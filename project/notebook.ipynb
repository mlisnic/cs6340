{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/maximlisnic/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import glob\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import metrics\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadInput(filename):\n",
    "\n",
    "    # Open file\n",
    "    file = open(filename, 'r')\n",
    "    contents = file.read()\n",
    "    file.close()\n",
    "\n",
    "    return contents\n",
    "\n",
    "def ReadKey(filename):\n",
    "\n",
    "    # Open file\n",
    "    file = open(filename, 'r')\n",
    "    lines = file.read().splitlines()\n",
    "    file.close()\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # Read lines and parse\n",
    "    for line in lines:\n",
    "        items = line.split(': ', 1)\n",
    "\n",
    "        slot = items[0]\n",
    "\n",
    "        if len(items) == 1:\n",
    "            continue\n",
    "        elif items[1] == '---':\n",
    "            continue\n",
    "        elif slot == 'TEXT':\n",
    "            continue\n",
    "        elif bool(re.match('^[0-9]+\\Z', items[1])):\n",
    "            answer = items[1]\n",
    "        else:\n",
    "            answer = items[1].split('\"')[1]\n",
    "\n",
    "        answer = nltk.word_tokenize(answer + ' .')[:-1]\n",
    "        slot_tags = ['B-' + slot] + ['I-' + slot]*(len(answer)-1)\n",
    "\n",
    "        entry = {'slot': slot_tags, 'answer': answer}\n",
    "        data.append(entry)\n",
    "\n",
    "    return data\n",
    "\n",
    "def ReadList(filename):\n",
    "    \"\"\"\n",
    "    read and parse the helper lists\n",
    "    \"\"\"\n",
    "\n",
    "    # Init output\n",
    "    data = []\n",
    "    \n",
    "    # Open file\n",
    "    file = open(filename, 'r')\n",
    "    lines = file.read().splitlines()\n",
    "    file.close()\n",
    "\n",
    "    # Read lines and parse\n",
    "    for line in lines:\n",
    "        data.append(line)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://www.geeksforgeeks.org/kmp-algorithm-for-pattern-searching/\n",
    "def KMPSearch(pat, txt):\n",
    "\tM = len(pat)\n",
    "\tN = len(txt)\n",
    "\n",
    "\tstarting_indices = []\n",
    "\n",
    "\tlps = [0]*M\n",
    "\tj = 0 # index for pat[]\n",
    "\n",
    "\tcomputeLPSArray(pat, M, lps)\n",
    "\n",
    "\ti = 0 # index for txt[]\n",
    "\twhile i < N:\n",
    "\t\tif pat[j] == txt[i]:\n",
    "\t\t\ti += 1\n",
    "\t\t\tj += 1\n",
    "\n",
    "\t\tif j == M:\n",
    "\t\t\tstarting_indices.append((i-j))\n",
    "\t\t\tj = lps[j-1]\n",
    "\n",
    "\t\telif i < N and pat[j] != txt[i]:\n",
    "\t\t\tif j != 0:\n",
    "\t\t\t\tj = lps[j-1]\n",
    "\t\t\telse:\n",
    "\t\t\t\ti += 1\n",
    "\t\n",
    "\treturn starting_indices\n",
    "\n",
    "def computeLPSArray(pat, M, lps):\n",
    "\tlen = 0 \n",
    "\n",
    "\tlps[0] \n",
    "\ti = 1\n",
    "\n",
    "\twhile i < M:\n",
    "\t\tif pat[i]== pat[len]:\n",
    "\t\t\tlen += 1\n",
    "\t\t\tlps[i] = len\n",
    "\t\t\ti += 1\n",
    "\t\telse:\n",
    "\t\t\tif len != 0:\n",
    "\t\t\t\tlen = lps[len-1]\n",
    "\t\t\telse:\n",
    "\t\t\t\tlps[i] = 0\n",
    "\t\t\t\ti += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildInitialData(text, key):\n",
    "    \n",
    "    # Tokenize text\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = [['PHI-2', 'PHI-1', 'PHI'] + nltk.word_tokenize(sentence) + ['OMEGA', 'OMEGA+1', 'OMEGA+2'] for sentence in sentences]\n",
    "    words = [word for sentence in words for word in sentence]\n",
    "\n",
    "    # Match all key tags\n",
    "    bio_tags = ['O'] * len(words)\n",
    "    for item in key:\n",
    "        answer_length = len(item['answer'])\n",
    "        indices = KMPSearch(item['answer'], words)\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "        for i in indices:\n",
    "            bio_tags[i:i+answer_length] = item['slot']\n",
    "\n",
    "    # Get POS tags\n",
    "    words = nltk.pos_tag(words)\n",
    "\n",
    "    # Build a dataframe\n",
    "    data = pd.DataFrame(\n",
    "        words,\n",
    "        columns = ['WORD', 'POS']\n",
    "    )\n",
    "    data['LABEL'] = bio_tags\n",
    "    data.loc[data['WORD'] == 'PHI', 'POS'] = 'PHIPOS'\n",
    "    data.loc[data['WORD'] == 'OMEGA', 'POS'] = 'OMEGAPOS'\n",
    "    data.loc[data['WORD'] == 'PHI-1', 'POS'] = 'PHI-1POS'\n",
    "    data.loc[data['WORD'] == 'OMEGA+1', 'POS'] = 'OMEGA+1POS'\n",
    "    data.loc[data['WORD'] == 'PHI-2', 'POS'] = 'PHI-2POS'\n",
    "    data.loc[data['WORD'] == 'OMEGA+2', 'POS'] = 'OMEGA+1POS'\n",
    "\n",
    "    return data\n",
    "\n",
    "def BuildFeatures(data, prefixes, prepositions, suffixes, locations_list):\n",
    "    \"\"\"\n",
    "    function to build the features\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Word attributes ------------------------------\n",
    "\n",
    "    data['LEMMA'] = (\n",
    "        data\n",
    "        .apply(\n",
    "            lambda x: lemmatizer.lemmatize(x['WORD']),\n",
    "            axis = 1\n",
    "         )\n",
    "    )\n",
    "\n",
    "    data['ABBR'] = (\n",
    "        data\n",
    "        .apply(\n",
    "            lambda x: x['WORD'].endswith('.')\n",
    "                      and bool(re.match('^[a-zA-Z.]+\\Z', x['WORD']))\n",
    "                      and bool(re.match('[a-zA-Z]', x['WORD']))\n",
    "                      and len(x['WORD']) in [2,3,4],\n",
    "            axis = 1\n",
    "            )\n",
    "    )\n",
    "\n",
    "    data['CAP'] = (\n",
    "        data\n",
    "        .apply(\n",
    "            lambda x: x['WORD'][0].isupper(),\n",
    "            axis = 1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    data['NUM'] = (\n",
    "        data\n",
    "        .apply(\n",
    "            lambda x: bool(re.match('[0-9]', x['WORD'])),\n",
    "            axis = 1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Adjacent words ------------------------------\n",
    "\n",
    "    data['NUM+1'] = (\n",
    "        data\n",
    "        ['NUM']\n",
    "        .shift(-1)\n",
    "    )\n",
    "\n",
    "    data['NUM-1'] = (\n",
    "        data\n",
    "        ['NUM']\n",
    "        .shift(1)\n",
    "    )\n",
    "\n",
    "    data['WORD+1'] = (\n",
    "        data\n",
    "        ['LEMMA']\n",
    "        .shift(-1)\n",
    "    )\n",
    "\n",
    "    data['WORD-1'] = (\n",
    "        data\n",
    "        ['LEMMA']\n",
    "        .shift(1)\n",
    "    )\n",
    "\n",
    "    data['WORD+2'] = (\n",
    "        data\n",
    "        ['LEMMA']\n",
    "        .shift(-2)\n",
    "    )\n",
    "\n",
    "    data['WORD-2'] = (\n",
    "        data\n",
    "        ['LEMMA']\n",
    "        .shift(2)\n",
    "    )\n",
    "\n",
    "    data['WORD+3'] = (\n",
    "        data\n",
    "        ['LEMMA']\n",
    "        .shift(-3)\n",
    "    )\n",
    "\n",
    "    data['WORD-3'] = (\n",
    "        data\n",
    "        ['LEMMA']\n",
    "        .shift(3)\n",
    "    )\n",
    "\n",
    "    data['POS+1'] = (\n",
    "        data\n",
    "        ['POS']\n",
    "        .shift(-1)\n",
    "    )\n",
    "\n",
    "    data['POS-1'] = (\n",
    "        data\n",
    "        ['POS']\n",
    "        .shift(1)\n",
    "    )\n",
    "\n",
    "    data['POS+2'] = (\n",
    "        data\n",
    "        ['POS']\n",
    "        .shift(-2)\n",
    "    )\n",
    "\n",
    "    data['POS-2'] = (\n",
    "        data\n",
    "        ['POS']\n",
    "        .shift(2)\n",
    "    )\n",
    "\n",
    "    data['POS+3'] = (\n",
    "        data\n",
    "        ['POS']\n",
    "        .shift(-3)\n",
    "    )\n",
    "\n",
    "    data['POS-3'] = (\n",
    "        data\n",
    "        ['POS']\n",
    "        .shift(3)\n",
    "    )\n",
    "\n",
    "    # Matching lists ------------------------------\n",
    "\n",
    "    data['LOC'] = (\n",
    "        data\n",
    "        .apply(\n",
    "            lambda x: x['WORD'].lower() in locations_list,\n",
    "            axis = 1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    data['PREF'] = (\n",
    "        data\n",
    "        .apply(\n",
    "            lambda x: x['WORD-1'] in prefixes,\n",
    "            axis = 1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    data['SUFF'] = (\n",
    "        data\n",
    "        .apply(\n",
    "            lambda x: x['WORD+1'] in suffixes,\n",
    "            axis = 1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Globals ------------------------------\n",
    "\n",
    "    data['LOWERCASE'] = (\n",
    "        data\n",
    "        .apply(lambda x: x['WORD'].lower(),\n",
    "               axis = 1)\n",
    "    )\n",
    "\n",
    "    data['GLOBCAP'] = (\n",
    "        data\n",
    "        .apply(\n",
    "            lambda x: bool(re.match('^[a-zA-Z]+\\Z', x['WORD']))\n",
    "                      and not x['WORD'].lower() in prepositions\n",
    "                      and x['WORD-1'] != 'PHI'\n",
    "                      and x['CAP'],\n",
    "            axis = 1\n",
    "        )\n",
    "    )\n",
    "    data['GLOBCAP'] = (\n",
    "        data\n",
    "        .groupby(['LOWERCASE'])\n",
    "        ['GLOBCAP']\n",
    "        .transform('any')\n",
    "    )\n",
    "\n",
    "    data['GLOBPREF'] = (\n",
    "        data\n",
    "        .apply(\n",
    "            lambda x: bool(re.match('^[a-zA-Z]+\\Z', x['WORD']))\n",
    "                      and x['WORD-1'] != 'PHI'\n",
    "                      and x['WORD-1'] in prefixes,\n",
    "            axis = 1\n",
    "        )\n",
    "    )\n",
    "    data['GLOBPREF'] = (\n",
    "        data\n",
    "        .groupby(['WORD'])\n",
    "        ['GLOBPREF']\n",
    "        .transform('any')\n",
    "    )\n",
    "\n",
    "    data['GLOBSUFF'] = (\n",
    "        data\n",
    "        .apply(\n",
    "            lambda x: bool(re.match('^[a-zA-Z]+\\Z', x['WORD']))\n",
    "                      and x['WORD+1'] != 'OMEGA'\n",
    "                      and x['WORD+1'] in suffixes,\n",
    "            axis = 1\n",
    "        )\n",
    "    )\n",
    "    data['GLOBSUFF'] = (\n",
    "        data\n",
    "        .groupby(['WORD'])\n",
    "        ['GLOBSUFF']\n",
    "        .transform('any')\n",
    "    )\n",
    "\n",
    "    # Clean up ------------------------------\n",
    "\n",
    "    data = (\n",
    "        data\n",
    "        .query('WORD != \"PHI\" & WORD != \"OMEGA\"')\n",
    "        .query('WORD != \"PHI-1\" & WORD != \"OMEGA+1\"')\n",
    "        .query('WORD != \"PHI-2\" & WORD != \"OMEGA+2\"')\n",
    "        .drop(columns = 'LOWERCASE')\n",
    "        .reset_index(drop = True)\n",
    "    )\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read helpers\n",
    "prefixes       = ReadList('./lists/prefixes.txt')\n",
    "prepositions   = ReadList('./lists/prepositions.txt')\n",
    "suffixes       = ReadList('./lists/suffixes.txt')\n",
    "locations      = pd.read_csv('./lists/locations.csv')\n",
    "locations_list = list(locations['country']) + list(locations['capital'])\n",
    "locations_list = [x.lower() for x in locations_list]\n",
    "\n",
    "all_docs = sorted(glob.glob('./data/development-docs/*'))\n",
    "all_keys = sorted(glob.glob('./data/development-anskeys/*'))\n",
    "\n",
    "data = []\n",
    "\n",
    "for f_doc, f_key in zip(all_docs, all_keys):\n",
    "    text = ReadInput(f_doc)\n",
    "    key = ReadKey(f_key)\n",
    "    x = BuildInitialData(text, key)\n",
    "    x['DOC'] = f_doc.split('/')[-1]\n",
    "    data.append(x)\n",
    "\n",
    "#data = pd.concat(data)\n",
    "#data = BuildFeatures(data, prefixes, prepositions, suffixes, locations_list)\n",
    "data = [BuildFeatures(sent, prefixes, prepositions, suffixes, locations_list) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For logit\n",
    "all_docs = data['DOC'].unique()\n",
    "train_docs = all_docs[0:300]\n",
    "test_docs = all_docs[300:]\n",
    "\n",
    "X_train = data.loc[:, data.columns != 'LABEL'][data['DOC'].isin(train_docs)]\n",
    "y_train = data[data['DOC'].isin(train_docs)]['LABEL']\n",
    "\n",
    "X_test = data.loc[:, data.columns != 'LABEL'][data['DOC'].isin(test_docs)]\n",
    "y_test = data[data['DOC'].isin(test_docs)]['LABEL']\n",
    "\n",
    "v = DictVectorizer(sparse=False)\n",
    "X_train_v = v.fit_transform(X_train.to_dict('records'))\n",
    "X_test_v = v.transform(X_test.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CRF\n",
    "train_docs = [doc.split('/')[-1] for doc in all_docs[0:300]]\n",
    "test_docs = [doc.split('/')[-1] for doc in all_docs[300:]]\n",
    "\n",
    "X_train = [sent.loc[:, sent.columns != 'LABEL'].to_dict('records') for sent in data if sent['DOC'][0] in train_docs]\n",
    "y_train = [sent['LABEL'] for sent in data if sent['DOC'][0] in train_docs]\n",
    "\n",
    "X_test = [sent.loc[:, sent.columns != 'LABEL'].to_dict('records') for sent in data if sent['DOC'][0] in test_docs]\n",
    "y_test = [sent['LABEL'] for sent in data if sent['DOC'][0] in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 460 ms, sys: 18.4 ms, total: 479 ms\n",
      "Wall time: 487 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 300,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 384 ms, total: 1min 12s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('test_crf_model.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 1000,\n",
       " 'scores': {},\n",
       " 'loss': 2984.904879,\n",
       " 'feature_norm': 47.521578,\n",
       " 'error_norm': 11.279807,\n",
       " 'active_features': 2407,\n",
       " 'linesearch_trials': 3,\n",
       " 'linesearch_step': 0.25,\n",
       " 'time': 0.121}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.logparser.last_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7faf714d6860>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('test_crf_model.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = [tagger.tag(sent) for sent in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractSlot(slot, pred, sent):\n",
    "\n",
    "    mask = [bool(re.match('[BI]-'+slot, tag)) for tag in pred]\n",
    "    tags = np.array(pred)[mask]\n",
    "    words = np.array([word['WORD'] for word in sent])[mask]\n",
    "\n",
    "    seen = []\n",
    "    results = []\n",
    "    for tag, word in zip(tags,words):\n",
    "        if tag[0] == 'B':\n",
    "            results.append(' '.join(seen))\n",
    "            seen = []\n",
    "            seen.append(word)\n",
    "        else:\n",
    "            seen.append(word)\n",
    "    results.append(' '.join(seen))\n",
    "    results = list(set([item for item in results if item != '']))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "\n",
    "for sent, pred in zip(X_test, yhat):\n",
    "\n",
    "    phrases = []\n",
    "    for slot in ['ACQUIRED', 'ACQBUS', 'ACQLOC', 'DLRAMT', 'PURCHASER', 'SELLER', 'STATUS']:\n",
    "        phrases.append(ExtractSlot(slot, pred, sent))\n",
    "    \n",
    "    doc_answer = {\n",
    "        'TEXT': sent[0]['DOC'],\n",
    "        'ACQUIRED': phrases[0],\n",
    "        'ACQBUS': phrases[1],\n",
    "        'ACQLOC': phrases[2],\n",
    "        'DLRAMT': phrases[3],\n",
    "        'PURCHASER': phrases[4],\n",
    "        'SELLER': phrases[5],\n",
    "        'STATUS': phrases[6],\n",
    "    }\n",
    "    answers.append(doc_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 19 epochs took 979 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 16.3min finished\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, tol=0.1, solver='sag', verbose=1, n_jobs=-1).fit(X_train_v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    B-ACQBUS       0.07      0.50      0.12         6\n",
      "    B-ACQLOC       0.14      0.38      0.20         8\n",
      "  B-ACQUIRED       0.43      0.58      0.49        73\n",
      "    B-DLRAMT       0.51      0.68      0.58        28\n",
      " B-PURCHASER       0.54      0.51      0.52       104\n",
      "    B-SELLER       0.05      0.40      0.09         5\n",
      "    B-STATUS       0.48      0.65      0.55        55\n",
      "    I-ACQBUS       0.15      0.54      0.24        26\n",
      "    I-ACQLOC       0.07      0.29      0.11         7\n",
      "  I-ACQUIRED       0.47      0.51      0.49       197\n",
      "    I-DLRAMT       0.34      0.59      0.43        34\n",
      " I-PURCHASER       0.49      0.52      0.50       181\n",
      "    I-SELLER       0.08      0.35      0.13        17\n",
      "    I-STATUS       0.35      0.62      0.45        60\n",
      "           O       0.98      0.95      0.96     10971\n",
      "\n",
      "    accuracy                           0.92     11772\n",
      "   macro avg       0.34      0.54      0.39     11772\n",
      "weighted avg       0.94      0.92      0.93     11772\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yhat = clf.predict(X_test_v)\n",
    "\n",
    "cr = metrics.classification_report(yhat, y_test)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "datahat = X_test.copy()\n",
    "datahat['pred'] = yhat\n",
    "datahat['true'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximlisnic/opt/miniconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "\n",
    "for doc in test_docs:\n",
    "    sent = datahat[datahat['DOC'] == doc][datahat['pred'] != 'O']\n",
    "    if len(sent) == 0:\n",
    "        continue\n",
    "    sent['type'] = (\n",
    "        sent\n",
    "        .apply(\n",
    "            lambda x: x['pred'].split('-')[1],\n",
    "            axis = 1\n",
    "            )\n",
    "    )\n",
    "\n",
    "    phrases = []\n",
    "    for TYPE in ['ACQUIRED', 'ACQBUS', 'ACQLOC', 'DLRAMT', 'PURCHASER', 'SELLER', 'STATUS']:\n",
    "        phrase = ' '.join(sent[sent['type'] == TYPE]['WORD'])\n",
    "        if phrase == '':\n",
    "            phrases.append('---')\n",
    "        else:\n",
    "            phrases.append(phrase)\n",
    "    \n",
    "    doc_answer = {\n",
    "        'TEXT': doc,\n",
    "        'ACQUIRED': phrases[0],\n",
    "        'ACQBUS': phrases[1],\n",
    "        'ACQLOC': phrases[2],\n",
    "        'DLRAMT': phrases[3],\n",
    "        'PURCHASER': phrases[4],\n",
    "        'SELLER': phrases[5],\n",
    "        'STATUS': phrases[6],\n",
    "    }\n",
    "    answers.append(doc_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TEXT': '16029',\n",
       " 'ACQUIRED': ['IPCO Corp', 'Prime Medical Products Inc'],\n",
       " 'ACQBUS': [],\n",
       " 'ACQLOC': [],\n",
       " 'DLRAMT': ['4.9 mln dlrs'],\n",
       " 'PURCHASER': [],\n",
       " 'SELLER': [],\n",
       " 'STATUS': []}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_stdout = sys.stdout\n",
    "\n",
    "with open('./scorer/test_results_crf.templates', 'w') as f:\n",
    "    sys.stdout = f \n",
    "    for answer in answers:\n",
    "        for key,value in answer.items():\n",
    "            if len(value) == 0:\n",
    "                print('%s: ---' % key)\n",
    "            elif key == 'TEXT':\n",
    "                print('%s: %s' % (key, value))\n",
    "            else:\n",
    "                for item in value:\n",
    "                    print('%s: \\\"%s\\\"' % (key, item))\n",
    "        print('')\n",
    "    sys.stdout = original_stdout\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gold file\n",
    "test_keys = ['./data/development-anskeys/' + num + '.key' for num in test_docs]\n",
    "original_stdout = sys.stdout\n",
    "\n",
    "with open('./scorer/test_gold.templates', 'w') as f:\n",
    "    sys.stdout = f \n",
    "    for filename in test_keys:\n",
    "        file = open(filename, 'r')\n",
    "        lines = file.read()\n",
    "        file.close()\n",
    "        print(lines)\n",
    "        print('')\n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "# Create gold file\n",
    "\n",
    "original_stdout = sys.stdout\n",
    "with open('./scorer/all_gold.templates', 'w') as f:\n",
    "    sys.stdout = f \n",
    "    for filename in all_keys:\n",
    "        file = open(filename, 'r')\n",
    "        lines = file.read()\n",
    "        file.close()\n",
    "        print(lines)\n",
    "        print('')\n",
    "    sys.stdout = original_stdout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build some input files\n",
    "\n",
    "############################ Docs ###################################\n",
    "# All -------------------\n",
    "original_stdout = sys.stdout\n",
    "with open('./all_docs.txt', 'w') as f:\n",
    "    sys.stdout = f \n",
    "    for filename in all_docs:\n",
    "        print(filename)\n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "# Train -------------------\n",
    "original_stdout = sys.stdout\n",
    "train_filenames = ['./data/development-docs/' + num for num in train_docs]\n",
    "with open('./train_docs.txt', 'w') as f:\n",
    "    sys.stdout = f \n",
    "    for filename in train_filenames:\n",
    "        print(filename)\n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "# Test -------------------\n",
    "original_stdout = sys.stdout\n",
    "test_filenames = ['./data/development-docs/' + num for num in test_docs]\n",
    "with open('./test_docs.txt', 'w') as f:\n",
    "    sys.stdout = f \n",
    "    for filename in test_filenames:\n",
    "        print(filename)\n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "############################ Keys ###################################\n",
    "# All -------------------\n",
    "original_stdout = sys.stdout\n",
    "with open('./all_keys.txt', 'w') as f:\n",
    "    sys.stdout = f \n",
    "    for filename in all_keys:\n",
    "        print(filename)\n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "# Train -------------------\n",
    "original_stdout = sys.stdout\n",
    "train_keys = ['./data/development-anskeys/' + num + '.key' for num in train_docs]\n",
    "with open('./train_keys.txt', 'w') as f:\n",
    "    sys.stdout = f \n",
    "    for filename in train_keys:\n",
    "        print(filename)\n",
    "    sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_stdout = sys.stdout\n",
    "\n",
    "with open('./all_docs.txt', 'w') as f:\n",
    "    sys.stdout = f \n",
    "    for filename in all_docs:\n",
    "        print(filename)\n",
    "        print('')\n",
    "    sys.stdout = original_stdout"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b1a3047961c48b7b8e536389294519daddc89a801ccbcbfe22b96f8998f95f9"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('py36': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
